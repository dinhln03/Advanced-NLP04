{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Assignment 2\n","\n","In this assignment, we're set to embark on an exciting journey to the cutting edge of machine learning methodologies. Specifically, we'll explore and apply state-of-the-art fine-tuning techniques to large language models. The techniques we'll delve into are not only robust but also resource-efficient, allowing us to perform the task of fine-tuning even on the free T4 GPUs available in a Kaggle notebook.\n","\n","Among the techniques we will explore is **Low-Rank Adaptation (LoRA)**, a novel method that has proven to be efficient and effective in adapting large pre-trained language models to specific tasks. LoRA is grounded in the hypothesis that updates to the weights during adaptation have a low \"intrinsic rank\", allowing us to constrain weight updates and reduce computational complexity, while preserving model performance.\n","\n","Complementing LoRA, we will also engage with **mixed-precision training**. This technique combines different numerical precisions to perform computations, aiming to maximize the computational power of modern GPUs. Mixed-precision training can accelerate model training, reduce memory requirements, and thus enable us to train larger, more powerful models.\n","\n","Finally, we will delve into **distributed training**, a must-know technique for handling very large models or datasets. With distributed training, we can leverage multiple GPUs or even multiple machines to collectively train a single model, effectively overcoming the limitations posed by the memory capacity of individual GPUs.\n","\n","By the end of this assignment, you should be well-acquainted with these cutting-edge techniques and be capable of integrating them into your own machine learning projects. Let's embark on this exciting journey into the vanguard of machine learning fine-tuning methodologies!"]},{"cell_type":"markdown","metadata":{},"source":["### Dataset\n","\n","The Stanford Alpaca dataset is part of a project that aims to build and share an instruction-following model called Alpaca. The dataset contains 52,000 examples used for fine-tuning the Alpaca model, with each example consisting of a unique instruction that the model should follow, an optional context or input for the task, and the corresponding output generated by the OpenAI's text-davinci-003 model. More information is available at the [Data release](https://github.com/tatsu-lab/stanford_alpaca/blob/main/README.md#data-release) and [Alpaca project page](https://crfm.stanford.edu/2023/03/13/alpaca.html).\n","\n","### Model\n","\n","The [Phi-2 Language Model from Hugging Face](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) represents a notable advancement in the field of AI, boasting 2.7 billion parameters and delivering top-tier performance among base language models with fewer than 13 billion parameters. It competes with or even exceeds the capabilities of models that are up to 25 times its size on various complex benchmarks.\n","\n","Given our technical constraints and resource limitations, we find the [GPTQ quantization](https://arxiv.org/abs/2210.17323) method to be an optimal choice for training. This approach allows us to maintain high-quality output while reducing the model's bit size through precise fine-tuning.\n","\n","We plan to apply GPTQ quantization to fine-tune the Phi-2-2.7B model, utilizing the Stanford Alpaca dataset. This approach mirrors the supervised fine-tuning phase used in developing models akin to ChatGPT, aiming to improve Phi-2-2.7B's proficiency in understanding instructions and executing tasks as directed by users.\n","\n","\n","\n","\n","### Initial setup\n","\n","To prepare the environment for our project, please execute the commands below:"]},{"cell_type":"markdown","metadata":{},"source":["#### Clone and Install Libraries (Do not run if you haved)"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:42:25.923033Z","iopub.status.busy":"2024-02-25T09:42:25.922155Z","iopub.status.idle":"2024-02-25T09:43:51.603953Z","shell.execute_reply":"2024-02-25T09:43:51.602861Z","shell.execute_reply.started":"2024-02-25T09:42:25.922989Z"},"scrolled":true,"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'Advanced-NLP04'...\n","remote: Enumerating objects: 77, done.\u001b[K\n","remote: Counting objects: 100% (77/77), done.\u001b[K\n","remote: Compressing objects: 100% (49/49), done.\u001b[K\n","remote: Total 77 (delta 30), reused 71 (delta 24), pack-reused 0\u001b[K\n","Unpacking objects: 100% (77/77), 3.32 MiB | 6.31 MiB/s, done.\n","/kaggle/working/Advanced-NLP04/assignment02\n","Collecting git+https://github.com/huggingface/accelerate.git (from -r requirements.txt (line 7))\n","  Cloning https://github.com/huggingface/accelerate.git to /tmp/pip-req-build-e7u2k4g1\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/accelerate.git /tmp/pip-req-build-e7u2k4g1\n","  Resolved https://github.com/huggingface/accelerate.git to commit f20445d4acc41f3b391f523691be8c81572ed1ac\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting git+https://github.com/huggingface/datasets.git (from -r requirements.txt (line 8))\n","  Cloning https://github.com/huggingface/datasets.git to /tmp/pip-req-build-k1tlxnte\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/datasets.git /tmp/pip-req-build-k1tlxnte\n","  Resolved https://github.com/huggingface/datasets.git to commit b775390900132834e5edf487f5cbbf1299af1d88\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting git+https://github.com/huggingface/peft.git (from -r requirements.txt (line 12))\n","  Cloning https://github.com/huggingface/peft.git to /tmp/pip-req-build-2c5lzrrv\n","  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/peft.git /tmp/pip-req-build-2c5lzrrv\n","  Resolved https://github.com/huggingface/peft.git to commit 1b3b7b5b2a931e0b49a39b078c5c065c89b088a8\n","  Installing build dependencies ... \u001b[?25ldone\n","\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n","\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25hCollecting gdown (from -r requirements.txt (line 1))\n","  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (0.1.99)\n","Requirement already satisfied: transformers>=4.28.0 in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (4.37.0)\n","Collecting loralib (from -r requirements.txt (line 4))\n","  Downloading loralib-0.1.2-py3-none-any.whl.metadata (15 kB)\n","Collecting bitsandbytes (from -r requirements.txt (line 5))\n","  Downloading bitsandbytes-0.42.0-py3-none-any.whl.metadata (9.9 kB)\n","Requirement already satisfied: appdirs in /opt/conda/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.4.4)\n","Collecting einops (from -r requirements.txt (line 9))\n","  Downloading einops-0.7.0-py3-none-any.whl.metadata (13 kB)\n","Collecting auto-gptq (from -r requirements.txt (line 10))\n","  Downloading auto_gptq-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n","Collecting optimum (from -r requirements.txt (line 11))\n","  Downloading optimum-1.17.1-py3-none-any.whl.metadata (18 kB)\n","Requirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (4.12.2)\n","Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (3.13.1)\n","Requirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (2.31.0)\n","Requirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown->-r requirements.txt (line 1)) (4.66.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (1.24.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.15.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from transformers>=4.28.0->-r requirements.txt (line 3)) (0.4.2)\n","Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes->-r requirements.txt (line 5)) (1.11.4)\n","Requirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0.dev0->-r requirements.txt (line 7)) (5.9.3)\n","Requirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate==0.28.0.dev0->-r requirements.txt (line 7)) (2.1.2)\n","Collecting pyarrow>=12.0.0 (from datasets==2.17.2.dev0->-r requirements.txt (line 8))\n","  Downloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n","Collecting pyarrow-hotfix (from datasets==2.17.2.dev0->-r requirements.txt (line 8))\n","  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.2.dev0->-r requirements.txt (line 8)) (0.3.7)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.2.dev0->-r requirements.txt (line 8)) (2.1.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.2.dev0->-r requirements.txt (line 8)) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.2.dev0->-r requirements.txt (line 8)) (0.70.15)\n","Collecting fsspec<=2023.10.0,>=2023.1.0 (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.17.2.dev0->-r requirements.txt (line 8))\n","  Downloading fsspec-2023.10.0-py3-none-any.whl.metadata (6.8 kB)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets==2.17.2.dev0->-r requirements.txt (line 8)) (3.9.1)\n","Collecting rouge (from auto-gptq->-r requirements.txt (line 10))\n","  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n","Collecting gekko (from auto-gptq->-r requirements.txt (line 10))\n","  Downloading gekko-1.0.6-py3-none-any.whl.metadata (2.9 kB)\n","Collecting coloredlogs (from optimum->-r requirements.txt (line 11))\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m677.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n","\u001b[?25hRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from optimum->-r requirements.txt (line 11)) (1.12)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (4.0.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers>=4.28.0->-r requirements.txt (line 3)) (4.9.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers>=4.28.0->-r requirements.txt (line 3)) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (2023.11.17)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0.dev0->-r requirements.txt (line 7)) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.28.0.dev0->-r requirements.txt (line 7)) (3.1.2)\n","Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from transformers[sentencepiece]>=4.26.0->optimum->-r requirements.txt (line 11)) (3.20.3)\n","Requirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown->-r requirements.txt (line 1)) (2.5)\n","Collecting humanfriendly>=9.1 (from coloredlogs->optimum->-r requirements.txt (line 11))\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets==2.17.2.dev0->-r requirements.txt (line 8)) (2023.4)\n","Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown->-r requirements.txt (line 1)) (1.7.1)\n","Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from rouge->auto-gptq->-r requirements.txt (line 10)) (1.16.0)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->optimum->-r requirements.txt (line 11)) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.28.0.dev0->-r requirements.txt (line 7)) (2.1.3)\n","Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n","Downloading loralib-0.1.2-py3-none-any.whl (10 kB)\n","Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading einops-0.7.0-py3-none-any.whl (44 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading auto_gptq-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (23.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.5/23.5 MB\u001b[0m \u001b[31m49.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading optimum-1.17.1-py3-none-any.whl (407 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m407.1/407.1 kB\u001b[0m \u001b[31m20.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading fsspec-2023.10.0-py3-none-any.whl (166 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.4/166.4 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pyarrow-15.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (38.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.3/38.3 MB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n","\u001b[?25hDownloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n","\u001b[?25hDownloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n","Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: accelerate, datasets, peft\n","  Building wheel for accelerate (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for accelerate: filename=accelerate-0.28.0.dev0-py3-none-any.whl size=282564 sha256=64550d55d23da1fde4a2ab22d7aa1d79f7deabc3664973b786125a92586110c2\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-yfr5842s/wheels/9c/a3/1e/47368f9b6575655fe9ee1b6350cfa7d4b0befe66a35f8a8365\n","  Building wheel for datasets (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for datasets: filename=datasets-2.17.2.dev0-py3-none-any.whl size=510243 sha256=6fb5e03cf2663c8cc0de84ee00d81beca0bbe9a2936995d8bb122c0228a1d18c\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-yfr5842s/wheels/57/f4/c4/53c677af89fec0ef3226c1e75a38367b37c2fa626f0544d3e4\n","  Building wheel for peft (pyproject.toml) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for peft: filename=peft-0.8.2-py3-none-any.whl size=187962 sha256=031383b24fb184d543e94e5d3ae163cc39f7b2288392fb90b8a2cb90895c02df\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-yfr5842s/wheels/d7/c7/de/1368fac8590e1b103ddc2ec2a28ad51d83aded1a3830e8a087\n","Successfully built accelerate datasets peft\n","Installing collected packages: rouge, pyarrow-hotfix, pyarrow, loralib, humanfriendly, gekko, fsspec, einops, coloredlogs, bitsandbytes, gdown, accelerate, datasets, peft, optimum, auto-gptq\n","  Attempting uninstall: pyarrow\n","    Found existing installation: pyarrow 11.0.0\n","    Uninstalling pyarrow-11.0.0:\n","      Successfully uninstalled pyarrow-11.0.0\n","  Attempting uninstall: fsspec\n","    Found existing installation: fsspec 2023.12.2\n","    Uninstalling fsspec-2023.12.2:\n","      Successfully uninstalled fsspec-2023.12.2\n","  Attempting uninstall: accelerate\n","    Found existing installation: accelerate 0.26.1\n","    Uninstalling accelerate-0.26.1:\n","      Successfully uninstalled accelerate-0.26.1\n","  Attempting uninstall: datasets\n","    Found existing installation: datasets 2.1.0\n","    Uninstalling datasets-2.1.0:\n","      Successfully uninstalled datasets-2.1.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","cudf 23.8.0 requires cubinlinker, which is not installed.\n","cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","cudf 23.8.0 requires ptxcompiler, which is not installed.\n","cuml 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","dask-cudf 23.8.0 requires cupy-cuda11x>=12.0.0, which is not installed.\n","apache-beam 2.46.0 requires dill<0.3.2,>=0.3.1.1, but you have dill 0.3.7 which is incompatible.\n","apache-beam 2.46.0 requires pyarrow<10.0.0,>=3.0.0, but you have pyarrow 15.0.0 which is incompatible.\n","beatrix-jupyterlab 2023.128.151533 requires jupyterlab~=3.6.0, but you have jupyterlab 4.0.11 which is incompatible.\n","cudf 23.8.0 requires cuda-python<12.0a0,>=11.7.1, but you have cuda-python 12.3.0 which is incompatible.\n","cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","cudf 23.8.0 requires protobuf<5,>=4.21, but you have protobuf 3.20.3 which is incompatible.\n","cudf 23.8.0 requires pyarrow==11.*, but you have pyarrow 15.0.0 which is incompatible.\n","cuml 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n","cuml 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n","dask-cuda 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n","dask-cuda 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n","dask-cuda 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","dask-cudf 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n","dask-cudf 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n","dask-cudf 23.8.0 requires pandas<1.6.0dev0,>=1.3, but you have pandas 2.1.4 which is incompatible.\n","gcsfs 2023.12.2.post1 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\n","raft-dask 23.8.0 requires dask==2023.7.1, but you have dask 2024.1.0 which is incompatible.\n","raft-dask 23.8.0 requires distributed==2023.7.1, but you have distributed 2024.1.0 which is incompatible.\n","s3fs 2023.12.2 requires fsspec==2023.12.2, but you have fsspec 2023.10.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed accelerate-0.28.0.dev0 auto-gptq-0.7.0 bitsandbytes-0.42.0 coloredlogs-15.0.1 datasets-2.17.2.dev0 einops-0.7.0 fsspec-2023.10.0 gdown-5.1.0 gekko-1.0.6 humanfriendly-10.0 loralib-0.1.2 optimum-1.17.1 peft-0.8.2 pyarrow-15.0.0 pyarrow-hotfix-0.6 rouge-1.0.1\n"]}],"source":["!git clone https://github.com/dinhln03/Advanced-NLP04.git\n","%cd Advanced-NLP04/assignment02\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{},"source":["#### RUN ONLY AS NEEDED (Execute this code if you have recently restarted the kernel).\n","\n","The following instructions are intended for situations where you encounter an Out of Memory error and need to restart the kernel. After restarting the kernel, execute the code in this cell to resume operations smoothly. Therefore, this should only be run if you've recently restarted the kernel."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%cd Advanced-NLP04/assignment02"]},{"cell_type":"markdown","metadata":{},"source":["## Part 1: Low-Rank Adaptation of Large Language Models for Efficient Fine-tuning"]},{"cell_type":"markdown","metadata":{},"source":["![](https://miro.medium.com/v2/resize:fit:730/1*D_i25E9dTd_5HMa45zITSg.png)\n","\n","Figure 1: LoRA method. We only train A and B.\n","\n","\n","### 1. Introduction\n","\n","**LoRA, or Low-Rank Adaptation**, is a technique for adapting large language models to specific tasks or domains more efficiently. It's based on the observation that as models get larger, the conventional approach of full fine-tuning becomes less feasible due to the large number of parameters involved.\n","\n","This process involves injecting the matrices into the dense layer's update, optimizing them for the specific adaptation task while the original pretrained model weights remain unchanged.\n","\n","Here are some of the key points of the LoRA technique:\n","\n","- **Freezing Pretrained Weights**: Instead of modifying all the parameters of a pretrained model during fine-tuning, LoRA freezes the pretrained weights. This means that the original model weights remain unchanged during the adaptation process.\n","\n","- **Rank Decomposition Matrices**: LoRA freezes the pretrained model weights and *injects trainable rank decomposition matrices* into each layer of the Transformer architecture. These matrices are used to adjust the output of each layer in a way that's specific to the adaptation task.\n","\n","- **Indirect Training of Dense Layers**: The rank decomposition matrices allow for the indirect training of each dense layer in the neural network. They are injected into the layer's update during the adaptation process and optimized to enhance the layer's performance on the specific task or domain.\n","\n","- **Significant Reduction in Trainable Parameters**: By focusing on these rank decomposition matrices instead of the entire set of model weights, LoRA greatly reduces the number of trainable parameters for downstream tasks. For instance, in the case of GPT-3, LoRA can reduce the number of trainable parameters by a factor of 10,000.\n","\n","- **Maintaining Model Performance**: Despite the significant reduction in the number of trainable parameters, the LoRA technique is designed to maintain or even improve the performance of the large language model on the specific task or domain.\n","\n","In summary, LoRA is a method that tackles the challenge of adapting large language models to specific tasks or domains in a more efficient and feasible way, making the fine-tuning process more manageable and less resource-intensive."]},{"cell_type":"markdown","metadata":{},"source":["### 2. Details\n","\n","The LoRA technique introduces a mathematical concept known as low-rank approximation into the fine-tuning process of large language models. Here's a mathematical description of the process:\n","\n","LoRA involves modifying the pre-trained weight matrix $\\mathbf{W}_0 \\in \\mathbb{R}^{d \\times k}$ of a neural network layer by introducing a low-rank parametrized update matrix $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$, where $\\mathbf{B} \\in \\mathbb{R}^{d \\times r}$, $\\mathbf{A} \\in \\mathbb{R}^{r \\times k}$, and $r \\ll \\min(d, k)$.\n","\n","During the adaptation process, $\\mathbf{W}_0$ is kept frozen, which means it does not receive any gradient updates. The trainable parameters are contained within $\\mathbf{A}$ and $\\mathbf{B}$, which form the low-rank update matrix $\\Delta \\mathbf{W}$.\n","\n","It's important to note that both $\\mathbf{W}_0$ and $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ are multiplied with the same input, and their respective output vectors are summed. If $\\mathbf{x}$ is the input and $\\mathbf{h} = \\mathbf{W}_0\\mathbf{x}$ is the output of the original weight matrix, the modified output is:\n","\n","$\\mathbf{h} = \\mathbf{W}_0\\mathbf{x} + \\Delta \\mathbf{W} \\mathbf{x} = \\mathbf{W}_0\\mathbf{x} + \\mathbf{B}\\mathbf{A}\\mathbf{x} = (\\mathbf{W}_0 + \\mathbf{B}\\mathbf{A})\\mathbf{x}$ \n","\n","in which $\\mathbf{W}_0 + \\mathbf{B}\\mathbf{A}$ is called **merge** operation. We will implement it in this assignment. \n","\n","At the beginning of training, we initialize $\\mathbf{A}$ with a random Gaussian distribution and $\\mathbf{B}$ with zero, such that $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ is zero, as shown in **Figure 1**. This ensures that the initial output of the model remains the same as in the pre-training phase, and the adaptation starts from the original model state. \n","\n","The low-rank update $\\Delta \\mathbf{W} = \\mathbf{B}\\mathbf{A}$ then evolves during training, helping to specialize the model for a specific task while keeping the number of trainable parameters manageable. Additionally, $\\Delta \\mathbf{W}$ is scaled by $\\frac{\\alpha}{r}$ where $\\alpha$ is a constant hyper-parameter. **(*)**\n","\n","This process is applied for each Linear layer of self-attention layer in the BLOOM language model, leading to an adapted model that's specialized for a specific task or domain, with significantly fewer trainable parameters than the original model. For example, with GPT-3 175B, VRAM consumption during training is reduced from 1.2TB to 350GB. If $r = 4$ and only the query and value projection matrices are adapted, the checkpoint size is reduced by approximately 10,000 times (from 350GB to 35MB). This allows training with significantly fewer GPUs and helps to avoid communication overhead.\n","\n","Another benefit is the ability to switch between tasks at a lower cost by only swapping the LoRA weights, as opposed to all parameters. This enables the creation of many customized models that can be swapped in and out on the fly on machines that store the pre-trained weights in VRAM."]},{"cell_type":"markdown","metadata":{},"source":["**(*)** The reason for scaling the update $\\Delta W x$ by $\\frac{\\alpha}{r}$ is primarily for easier optimization.\n","\n","Consider the scenario where the rank $r$ changes during training. If you were to increase or decrease $r$, without this scaling factor, it would significantly affect the magnitude of the weight updates and thereby the learning dynamics of the model. In other words, changing $r$ would mean that you need to retune the learning rate or other hyperparameters, which is a laborious and time-consuming task.\n","\n","By scaling the updates by $\\frac{\\alpha}{r}$, the authors make the learning process more robust to changes in $r$. $\\alpha$ is a constant, so this scaling factor effectively normalizes the magnitude of the updates relative to the rank of the low-rank approximation.\n","\n","This way, even when $r$ changes, the overall scale of the updates remains approximately constant, meaning you can use the same learning rate and other hyperparameters. This is advantageous because it makes the training process more efficient and less sensitive to the choice of $r$.\n","\n","Keep in mind that this is a heuristic and it may not always provide the optimal solution for every problem or dataset, but it is a practical choice that often works well in practice."]},{"cell_type":"markdown","metadata":{},"source":["### 3. Implementation\n","\n","Let's break down the code, please take a look at the `lora_layer.py` file. The main components are:\n","\n","- **LoraLayer** class: This is a base class that provides common functionality for both linear and embedding layers using the LoRA technique. It keeps track of LoRA parameters including the rank **r** and two sets of weights **lora_A** and **lora_B** (or **lora_embedding_A** and **lora_embedding_B** for the embedding layer). Two methods, **update_layer** and **update_layer_embedding**, are defined to update these parameters for linear and embedding layers, respectively.\n","\n","- **Linear** and **Embedding** classes: These classes extend their corresponding PyTorch classes (**nn.Linear** and **nn.Embedding**) and the **LoraLayer** class. They initialize their superclasses as well as the LoRA parameters, and overwrite the **merge**, **unmerge**, and **forward** methods to implement the LoRA technique. The **merge** method combines the original weights of the layer with the LoRA weights, and **unmerge** undoes this operation. The **forward** method applies the layer operation either with or without the LoRA technique, depending on whether LoRA is enabled.\n","\n","This assignment is heavily based on the internal codebase of 🤗 PEFT library. 🤗 PEFT, or **Parameter-Efficient Fine-Tuning (PEFT)**, is a library for efficiently adapting pre-trained language models (PLMs) to various downstream applications without fine-tuning all the model’s parameters. Recent state-of-the-art PEFT techniques achieve performance comparable to that of full fine-tuning.\n","\n","If you are new to PEFT, get started by reading the [Quicktour](https://huggingface.co/docs/peft/quicktour) guide and conceptual guides for [LoRA](https://huggingface.co/docs/peft/conceptual_guides/lora) methods."]},{"cell_type":"markdown","metadata":{},"source":["#### Q1: Implement the Merge operation in LoRA (15 points)\n","In the provided `lora_layer.py` file, your task is to complete the `merge` method within the `Linear` class. As a useful reference, consider the already implemented `merge` method in the `Embedding` class. This should provide a clear guide on how to approach this task.\n","\n","#### Q2: Implement the Forward Pass in LoRA (15 points)\n","In the provided `lora_layer.py` file, your task is to complete the `forward` method within the `Linear` class. As a useful reference, consider the already implemented `forward` method in the `Embedding` class. This should provide a clear guide on how to approach this task.\n","\n","#### Q3: Construct the LoRA Model and Dataloaders for Training (20 points)\n","In the provided `train.py` file, your task is to complete the `load_pretrained_model` function and `prepare_dataloader`. The aforementioned PEFT's Quicktour guide and LoRA's conceptual guide can be useful references. Note, the specific details related to distributed training can be overlooked at this stage.\n","\n","Once you've finished the implementation, it's time to train your LoRA model. Congratulations!"]},{"cell_type":"markdown","metadata":{},"source":["##### Train with sample data"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:44:12.433070Z","iopub.status.busy":"2024-02-25T09:44:12.432642Z","iopub.status.idle":"2024-02-25T09:45:26.244091Z","shell.execute_reply":"2024-02-25T09:45:26.242932Z","shell.execute_reply.started":"2024-02-25T09:44:12.433033Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-02-25 09:44:24.085944: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 09:44:24.086079: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 09:44:24.251217: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","config.json: 100%|█████████████████████████| 1.59k/1.59k [00:00<00:00, 7.02MB/s]\n","configuration_phi.py: 100%|████████████████| 2.03k/2.03k [00:00<00:00, 10.3MB/s]\n","A new version of the following files was downloaded from https://huggingface.co/TheBloke/phi-2-GPTQ:\n","- configuration_phi.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","modeling_phi.py: 100%|█████████████████████| 33.4k/33.4k [00:00<00:00, 3.71MB/s]\n","A new version of the following files was downloaded from https://huggingface.co/TheBloke/phi-2-GPTQ:\n","- modeling_phi.py\n",". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n","model.safetensors: 100%|████████████████████| 1.84G/1.84G [00:06<00:00, 268MB/s]\n","generation_config.json: 100%|█████████████████| 69.0/69.0 [00:00<00:00, 341kB/s]\n","trainable params: 430080 || all params: 262794240 || trainable%: 0.16365655502951662\n","tokenizer_config.json: 100%|███████████████| 7.34k/7.34k [00:00<00:00, 27.6MB/s]\n","vocab.json: 100%|████████████████████████████| 798k/798k [00:00<00:00, 19.6MB/s]\n","merges.txt: 100%|████████████████████████████| 456k/456k [00:00<00:00, 10.9MB/s]\n","tokenizer.json: 100%|██████████████████████| 2.11M/2.11M [00:00<00:00, 25.7MB/s]\n","added_tokens.json: 100%|███████████████████| 1.08k/1.08k [00:00<00:00, 4.52MB/s]\n","special_tokens_map.json: 100%|████████████████| 99.0/99.0 [00:00<00:00, 469kB/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Completed to load config & tokenizer\n","Load dataset....\n","Generating train split: 35 examples [00:00, 4945.45 examples/s]\n","Map: 100%|███████████████████████████████| 29/29 [00:03<00:00,  7.32 examples/s]\n","Map: 100%|████████████████████████████████| 6/6 [00:00<00:00, 482.43 examples/s]\n","Creating json from Arrow format: 100%|████████████| 1/1 [00:00<00:00, 48.49ba/s]\n","epoch = 1 | avg_train_loss = 2.1005614059312 | eval_loss = 1.8966246843338013   \n","Done saved at epoch_1_checkpoint\n","epoch = 2 | avg_train_loss = 2.134644567966461 | eval_loss = 1.8800325791041057 \n","Done saved at epoch_2_checkpoint\n","epoch = 3 | avg_train_loss = 2.063167359147753 | eval_loss = 1.8670399983723958 \n","Done saved at epoch_3_checkpoint\n"]}],"source":["# train with sample dataset\n","!DEBUG=true python train.py"]},{"cell_type":"markdown","metadata":{},"source":["##### Challenge 1: Will LoRA enhance inference speed? (5 points)\n","\n","*No, because after merging lora weight with model weight, the number of parameters remain the same.*\n","\n","\n","##### Challenge 2: Will LoRA improve training speed? (5 points)\n","\n","*Yes, because we only need to train only the smaller number of parameters of adapter layer.*\n"]},{"cell_type":"markdown","metadata":{},"source":["## Part 2: Mixed precision training\n","\n","The paper \"Mixed Precision Training\" is a game-changer in the world of deep learning. It introduces a method that combines different numerical precisions (like 32-bit and 16-bit) during model training. By using lower precision for certain parts of the training process, such as weight updates, we can speed up computations and reduce memory requirements without sacrificing accuracy. This technique leverages the increased computational power of modern GPUs and accelerators to achieve impressive results.\n","\n","### Implementation\n","\n","#### Q4: Implement Mixed Precision Training (15 points)\n","In the provided `train.py` file, your objective is to enable mixed precision training. To achieve this, complete the assignment of the `mixed_precision_dtype`, `self.ctx` and `self.gradscaler`. You may have to modify the `_run_batch` and `_run_epoch` using `self.gradscaler` in case you are using `mixed_precision_dtype` of `torch.float16`. If you paid close attention to the coding session during week 6, you should find this task straightforward.\n","\n","Once you have carried out these steps, proceed to execute the following cell to train your LoRA model with mixed precision training. You should observe significant speed improvement in training."]},{"cell_type":"code","execution_count":9,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:48:30.831868Z","iopub.status.busy":"2024-02-25T09:48:30.831308Z","iopub.status.idle":"2024-02-25T09:48:30.849428Z","shell.execute_reply":"2024-02-25T09:48:30.848421Z","shell.execute_reply.started":"2024-02-25T09:48:30.831828Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Overwriting train.py\n"]}],"source":["%%writefile train.py\n","import os\n","import torch\n","from tqdm import tqdm\n","\n","\n","from peft import LoraConfig, get_peft_model\n","from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, DataCollatorForSeq2Seq\n","\n","from contextlib import nullcontext\n","\n","from torch.nn.parallel import DistributedDataParallel as DDP\n","from torch.distributed import init_process_group, destroy_process_group\n","import torch.distributed as dist\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.utils.data import DataLoader, SequentialSampler\n","\n","\n","from lora_model import LoraModelForCasualLM\n","from utils.common import download_from_driver\n","from prepare_data import create_datasets\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","torch.manual_seed(42)\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","\n","class Trainer:\n","    def __init__(self,\n","                 model,\n","                 tokenizer,\n","                 gpu_id: int,\n","                 is_ddp_training: bool = True,\n","                 output_dir: str = 'checkpoints/',\n","                 num_epochs: int = 10,\n","                 max_length: int = 128,\n","                 batch_size: int = 8,\n","                 mixed_precision_dtype=None,\n","                 gradient_accumulation_steps: int = 16):\n","        \"\"\"\n","        Initialize the Trainer class.\n","\n","        Args:\n","            model: Pretrained model object.\n","            tokenizer: Tokenizer object for text processing.\n","            num_epochs: Number of training epochs.\n","            max_length: Maximum sequence length.\n","            batch_size: Training batch size.\n","            gpu_id: GPU ID for training.\n","        \"\"\"\n","\n","        self.num_epochs = num_epochs\n","        self.max_length = max_length\n","        self.batch_size = batch_size\n","        self.output_dir = output_dir\n","        self.tokenizer = tokenizer\n","        self.is_ddp_training = is_ddp_training\n","\n","        self.gpu_id = gpu_id\n","        self.model = model.to(f\"cuda:{self.gpu_id}\")\n","        self.gradient_accumulation_steps = gradient_accumulation_steps\n","\n","        self.mixed_precision_dtype = mixed_precision_dtype\n","        self.ctx = None\n","        self.gradscaler = None\n","\n","        # set mixed precision context\n","        self.set_mixed_precision_context(mixed_precision_dtype)\n","\n","    def set_mixed_precision_context(self, mixed_precision_dtype):\n","        \n","        # TODO: Setup mixed precision training context\n","\n","        if mixed_precision_dtype is None:\n","            \n","            # If 'mixed_precision_dtype' is None, use 'nullcontext',\n","            self.ctx = nullcontext()\n","\n","        else:\n","        \n","            # TODO Otherwise, use 'torch.amp.autocast' context with the specified dtype, and initialize GradScaler if mixed_precision_dtype is float16.\n","            \n","            self.ctx = torch.autocast(device_type='cuda',dtype=mixed_precision_dtype)\n","            self.gradscaler = torch.cuda.amp.GradScaler(enabled =True)\n","\n","    def _set_ddp_training(self):\n","\n","        # TODO: Initialize the DistributedDataParallel wrapper for the model.\n","        # You would need to pass the model and specify the device IDs\n","        # and output device for the data parallelism.\n","\n","        ### YOUR CODE HERE ###\n","\n","        self.model = DDP(self.model, device_ids=[self.gpu_id], output_device=self.gpu_id)\n","\n","    def _run_batch(self, batch):\n","        \"\"\"\n","        Run a single training batch.\n","\n","        Args:\n","            batch: Batch data.\n","\n","        Returns:\n","            Loss value for the batch.\n","        \"\"\"\n","\n","        with self.ctx :\n","            outputs = self.model(**batch)\n","            loss = outputs.loss / self.gradient_accumulation_steps  # Normalize loss\n","        loss_val = loss.item()\n","\n","        # TODO: If 'mixed_precision_dtype' is torch.float16, you have to modify the backward using the gradscaler.\n","        if self.mixed_precision_dtype == torch.float16:\n","            ### YOUR CODE HERE ###\n","            self.gradscaler.scale(loss).backward()\n","            \n","            pass\n","        else:\n","            loss.backward()\n","\n","        return loss_val\n","\n","    def _run_epoch(self, train_dataloader, epoch):\n","        \"\"\"\n","        Run a single training epoch.\n","\n","        Args:\n","            train_loader: Training data loader.\n","            epoch: Current epoch number.\n","\n","        Returns:\n","            Total loss value for the epoch.\n","        \"\"\"\n","\n","        epoch_loss = 0\n","        self.model.train()\n","\n","        if _is_master_process():\n","            train_progress_bar = tqdm(\n","                train_dataloader, desc=f\"Epoch {epoch + 1} [Training]\", position=0, leave=False)\n","        else:\n","            train_progress_bar = train_dataloader\n","\n","        # Add counter for gradient accumulation\n","        steps = 0\n","        self.optimizer.zero_grad()  # Reset gradients at the beginning of each epoch\n","        for step, batch in enumerate(train_progress_bar):\n","            steps += 1\n","            batch = {key: value.to(self.gpu_id)\n","                     for key, value in batch.items()}\n","            loss = self._run_batch(batch)\n","            epoch_loss += loss\n","\n","            # Perform optimizer step and reset gradients after accumulating enough gradients\n","            if steps % self.gradient_accumulation_steps == 0:\n","\n","                # If 'mixed_precision_dtype' is torch.float16, you have to modify the gradient update step using the gradscaler.\n","                if self.mixed_precision_dtype == torch.float16:\n","\n","                    ### YOUR CODE HERE ###\n","                    \n","                    # TODO: optimizer step\n","                    self.gradscaler.step(self.optimizer)\n","\n","                    # TODO: update scaler factor\n","                    self.gradscaler.update()\n","\n","                    pass\n","                else:\n","                    self.optimizer.step()\n","                self.optimizer.zero_grad()\n","\n","                torch.cuda.empty_cache()\n","        epoch_loss /= (len(train_dataloader) /\n","                       self.gradient_accumulation_steps)\n","        return epoch_loss\n","\n","    def _save_checkpoint(self, epoch):\n","        path_dir = f\"{self.output_dir}/epoch_{epoch}\"\n","\n","        # check path_dir exited\n","        if not os.path.exists(path_dir):\n","            os.makedirs(path_dir)\n","\n","        # save checkpoints\n","        if self.is_ddp_training and _is_master_process():\n","            self.model.module.save_pretrained(f'epoch_{epoch}_checkpoint')\n","        else:\n","            self.model.save_pretrained(f'epoch_{epoch}_checkpoint')\n","\n","        print(\"Done saved at\", f'epoch_{epoch}_checkpoint')\n","\n","    def prepare_dataloader(self, train_dataset, eval_dataset):\n","\n","        # TODO: Prepare the training DataLoader. Initialize 'DataLoader' with 'train_dataset'\n","        # and the appropriate 'batch_size'.\n","        # Depending on whether the training is distributed (is_ddp_training),\n","        # use 'DistributedSampler' for 'sampler' argument, else use 'None'.\n","        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\"\n","        # Also add drop_last to True.\n","\n","        ### YOUR CODE HERE ###\n","\n","        sampler = DistributedSampler(train_dataset) if self.is_ddp_training else None\n","        data_trainloader = DataLoader(\n","        train_dataset,\n","        batch_size=self.batch_size,\n","        sampler=sampler,\n","        collate_fn=DataCollatorForSeq2Seq(\n","            self.tokenizer,\n","            pad_to_multiple_of=8,\n","            return_tensors=\"pt\",\n","        ),\n","        drop_last=True,\n","    )\n","\n","        # TODO: Prepare the evaluation DataLoader. Initialize 'DataLoader' with 'eval_dataset',\n","        # the appropriate 'batch_size', and 'SequentialSampler' for 'sampler'.\n","        # Use 'DataCollatorForSeq2Seq' for 'collate_fn', passing 'tokenizer', padding settings and pad_to_multiple_of to 8, and return_tensors=\"pt\".\n","        # Also add drop_last to True.\n","\n","        ### YOUR CODE HERE ###\n","\n","        sampler = SequentialSampler(eval_dataset)\n","        data_testloader = DataLoader(\n","        eval_dataset,\n","        batch_size=self.batch_size,\n","        sampler=sampler,\n","        collate_fn=DataCollatorForSeq2Seq(\n","            self.tokenizer,\n","            pad_to_multiple_of=8,\n","            return_tensors=\"pt\",\n","        ),\n","        drop_last=True,\n","    )\n","\n","\n","        return data_trainloader, data_testloader\n","\n","    def _eval(self, eval_dataloader, epoch: int):\n","        avg_loss = 0\n","        model.eval()\n","        if _is_master_process():\n","            eval_progress_bar = tqdm(\n","                eval_dataloader, desc=f\"Epoch {epoch + 1} [Evaluation]\", position=0, leave=False)\n","        else:\n","            eval_progress_bar = eval_dataloader\n","\n","\n","        for batch in eval_progress_bar:\n","            with self.ctx:\n","                with torch.no_grad():\n","                    if not self.is_ddp_training:\n","                        outputs = self.model(**batch.to(self.gpu_id))\n","                    else:\n","                        outputs = self.model(**batch)\n","            avg_loss += outputs.loss.item()\n","        avg_loss = avg_loss/(len(eval_dataloader))\n","        return avg_loss\n","\n","    def run(self, data_path: str, size_valid_set: int = 0.25, seed: int = 123):\n","        \"\"\"\n","        Run the training process.\n","\n","        Returns:\n","            None\n","        \"\"\"\n","        train_dataset, eval_dataset = create_datasets(\n","            tokenizer=self.tokenizer,\n","            max_length=self.max_length,\n","            data_path=data_path,\n","            size_valid_set=size_valid_set,\n","            seed=seed\n","        )\n","\n","        train_dataloader, eval_dataloader = self.prepare_dataloader(\n","            train_dataset, eval_dataset)\n","\n","        if self.is_ddp_training:\n","            self._set_ddp_training()\n","\n","        self.optimizer = torch.optim.AdamW(\n","            self.model.parameters(), lr=learning_rate)\n","\n","        for epoch in range(self.num_epochs):\n","\n","            if self.is_ddp_training:\n","                train_dataloader.sampler.set_epoch(epoch)\n","\n","            train_loss = self._run_epoch(train_dataloader, epoch)\n","            if self.is_ddp_training:\n","                dist.barrier() \n","            if _is_master_process() or (epoch == self.num_epochs - 1):\n","                eval_loss = self._eval(\n","                    eval_dataloader=eval_dataloader, epoch=epoch)\n","\n","                print(\n","                    f\"epoch = {epoch+1} | avg_train_loss = {train_loss} | eval_loss = {eval_loss}\")\n","            \n","            if _is_master_process():\n","                self._save_checkpoint(epoch=epoch+1)\n","\n","def load_tokenizer_from_pretrained_model(model_path):\n","\n","    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n","    architecture = config.architectures[0]\n","    tokenizer = AutoTokenizer.from_pretrained(\n","        model_path, trust_remote_code=True, device_map={\"\": torch.device(f\"cuda:{0}\")})\n","    tokenizer.pad_token = tokenizer.eos_token\n","    if _is_master_process():\n","        print('Completed to load config & tokenizer')\n","\n","    if \"Llama\" in architecture:\n","        if _is_master_process():\n","            print(\"Setting EOS, BOS, UNK, and PAD tokens for LLama tokenizer\")\n","        tokenizer.add_special_tokens(\n","            {\n","                \"eos_token\": \"</s>\",\n","                \"bos_token\": \"</s>\",\n","                \"unk_token\": \"</s>\",\n","            }\n","        )\n","        tokenizer.pad_token_id = (\n","            0  # unk. we want this to be different from the eos token\n","        )\n","\n","    return tokenizer\n","\n","\n","def _is_master_process():\n","    ddp_rank = int(os.environ['RANK'])\n","    return ddp_rank == 0\n","\n","\n","def load_pretrained_model(local_rank, model_path: str = \"\"):\n","    # TODO: Load a pretrained AutoModelForCausalLM from the 'model_path'.\n","    # Make sure to set 'device_map' to '{\"\": torch.device(f\"cuda:{local_rank}\")}' for DDP training\n","    # and trust_remote_code=True.\n","\n","    ### YOUR CODE HERE ###\n","    model = AutoModelForCausalLM.from_pretrained(\n","        model_path, trust_remote_code=True, device_map={\"\": torch.device(f\"cuda:{local_rank}\")})\n","\n","    # TODO: Create a LoraConfig with the parameters: \n","    # r=4, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=['lm_head.linear', 'transformer.embd.wte'].\n","    # We will then use the config to initialize a LoraModelForCasualLM with the loaded model.\n","    \n","    ### YOUR CODE HERE ###\n","    lora_config = LoraConfig(\n","        r=4, lora_alpha=8, lora_dropout=0.05, bias=\"none\", task_type=\"CAUSAL_LM\", target_modules=['lm_head.linear', 'transformer.embd.wte'])\n","    # TODO: Create LoRA model\n","    # model = get_peft_model(model, lora_config)\n","    model = LoraModelForCasualLM(model, lora_config)\n","\n","    if _is_master_process():\n","        model.print_trainable_parameters()\n","\n","    return model\n","\n","\n","if __name__ == \"__main__\":\n","    OUTPUT_DIR = \"checkpoints/\"\n","\n","    backend = \"nccl\"\n","    model_path = 'TheBloke/phi-2-GPTQ'\n","    if os.environ.get(\"DEBUG\"):\n","        data_path = \"test_data.json\"\n","    else:\n","        data_path = 'alpaca_data.json'\n","\n","    size_valid_set = 0.15\n","    max_length = 128\n","    num_epochs = 3\n","    batch_size = 2\n","    gradient_accumulation_steps = 8\n","\n","    learning_rate = 3e-4\n","    lr_scheduler_type = 'cosine'\n","    num_warmup_steps = 100\n","    weight_decay = 0.06\n","\n","    seed = 0\n","    log_freq = 1\n","    eval_freq = 150\n","\n","    distributed_strategy = \"ddp\" if os.environ.get(\"ON_DDP\") else \"no\"\n","\n","    if distributed_strategy == \"ddp\":\n","\n","        # TODO: Initialize the process group for distributed data parallelism with nccl backend.\n","        init_process_group(backend=backend)\n","        # After that, you should set the 'local_rank' from the environment variable 'LOCAL_RANK'.\n","        local_rank = int(os.environ['LOCAL_RANK'])\n","        # Initialize the process group\n","        \n","\n","        ### YOUR CODE HERE ###\n","        \n","        pass\n","    else:\n","        os.environ['RANK'] = '0'\n","        local_rank = 0\n","\n","    # Prepare model\n","    model = load_pretrained_model(local_rank, model_path=model_path)\n","    \n","    # Get tokenizer\n","    tokenizer = load_tokenizer_from_pretrained_model(model_path=model_path)\n","\n","    # prepare trainer\n","    trainer = Trainer(\n","        model=model,\n","        num_epochs=num_epochs,\n","        max_length=max_length,\n","        batch_size=batch_size,\n","        gpu_id=local_rank,\n","        \n","        mixed_precision_dtype=torch.float16 if os.environ.get(\"ON_MP\") else None,\n","        \n","        tokenizer=tokenizer,\n","        output_dir=OUTPUT_DIR,\n","        is_ddp_training=True if distributed_strategy == \"ddp\" else False,\n","        gradient_accumulation_steps=gradient_accumulation_steps,\n","    )\n","\n","    # set ddp for wraping model\n","    # execute trainer\n","    trainer.run(\n","        data_path=data_path,\n","        size_valid_set=size_valid_set,\n","        seed=seed\n","    )\n","\n","    if distributed_strategy == \"ddp\":\n","        destroy_process_group()"]},{"cell_type":"markdown","metadata":{},"source":["##### Train with sample data"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:45:47.870247Z","iopub.status.busy":"2024-02-25T09:45:47.868798Z","iopub.status.idle":"2024-02-25T09:46:27.260220Z","shell.execute_reply":"2024-02-25T09:46:27.259046Z","shell.execute_reply.started":"2024-02-25T09:45:47.870201Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["2024-02-25 09:45:53.689168: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 09:45:53.689249: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 09:45:53.690758: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","trainable params: 430080 || all params: 262794240 || trainable%: 0.16365655502951662\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Completed to load config & tokenizer\n","Load dataset....\n","Map: 100%|██████████████████████████████| 29/29 [00:00<00:00, 912.43 examples/s]\n","Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 297.70ba/s]\n","epoch = 1 | avg_train_loss = 2.1192559685025896 | eval_loss = 2.0365958213806152\n","Done saved at epoch_1_checkpoint\n","epoch = 2 | avg_train_loss = 2.1197956459862843 | eval_loss = 2.0365958213806152\n","Done saved at epoch_2_checkpoint\n","epoch = 3 | avg_train_loss = 2.1122058970587596 | eval_loss = 2.0365958213806152\n","Done saved at epoch_3_checkpoint\n"]}],"source":["# mixed precision training with sample dataset\n","!DEBUG=true ON_MP=true python train.py "]},{"cell_type":"markdown","metadata":{},"source":["## Part 3: Distributed Training with DistributedDataParallel\n","\n","When it comes to training large language models, like those used for NLP tasks, the computational requirements can be ridiculously expensive. These models often have billions of parameters and require vast amounts of data to train effectively. This is where distributed training, and more specifically DistributedDataParallel (DDP), comes into play.\n","\n","Training large language models on a single GPU can be extremely time-consuming and sometimes outright impossible due to memory limitations. DDP allows us to train these models across multiple GPUs, and even across several machines. This not only speeds up the process but also allows us to train much larger models than would be possible on a single GPU.\n","\n","By dividing the model and the dataset across multiple GPUs, each with its own subset of data, we can train in parallel. This significantly reduces the time required to train these large models. Furthermore, the synchronization of model parameters after each forward and backward pass ensures consistency and accuracy across all model replicas.\n","\n","In this sections, we will utilize DistributedDataParallel for training large language models. Let's dive in!\n","\n","### Implementation\n","\n","#### Q5: Setup environment for DDP (25 points)\n","\n","In the provided `train.py` file, your mission is to enable distributed training utilizing the `DistributedDataParallel` (DDP) module from PyTorch. This task involves selecting the correct `distributed_strategy`, initializing the process group, establishing the local rank, and filling out the `_set_ddp_training` function. Furthermore, you are required to adapt the `load_pretrained_model` function and the `prepare_dataloader` method to be compatible with DDP training.\n","\n","Once you have carried out these steps, complete the `torchrun` command below to execute the following cell to train your LoRA model on Kaggle GPU T4 x2."]},{"cell_type":"markdown","metadata":{},"source":["##### Train with sample data"]},{"cell_type":"code","execution_count":10,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:48:45.413792Z","iopub.status.busy":"2024-02-25T09:48:45.413111Z","iopub.status.idle":"2024-02-25T09:49:34.085444Z","shell.execute_reply":"2024-02-25T09:49:34.084340Z","shell.execute_reply.started":"2024-02-25T09:48:45.413758Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-02-25 09:48:48,321] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n","[2024-02-25 09:48:48,321] torch.distributed.run: [WARNING] \n","[2024-02-25 09:48:48,321] torch.distributed.run: [WARNING] *****************************************\n","[2024-02-25 09:48:48,321] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n","[2024-02-25 09:48:48,321] torch.distributed.run: [WARNING] *****************************************\n","2024-02-25 09:48:53.416222: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 09:48:53.416278: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 09:48:53.417877: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-25 09:48:53.679539: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 09:48:53.679597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 09:48:53.681039: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","trainable params: 430080 || all params: 262794240 || trainable%: 0.16365655502951662\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Load dataset....\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Completed to load config & tokenizer\n","Load dataset....\n","Map: 100%|██████████████████████████████| 29/29 [00:00<00:00, 762.45 examples/s]\n","Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 386.64ba/s]\n","Map: 100%|██████████████████████████████| 29/29 [00:00<00:00, 883.02 examples/s]\n","Creating json from Arrow format: 100%|███████████| 1/1 [00:00<00:00, 397.87ba/s]\n","epoch = 1 | avg_train_loss = 2.200200251170567 | eval_loss = 1.916185975074768  \n","Done saved at epoch_1_checkpoint\n","epoch = 2 | avg_train_loss = 2.1507737806865146 | eval_loss = 1.916185975074768 \n","Done saved at epoch_2_checkpoint\n","Epoch 3 [Evaluation]:  67%|███████████████▎       | 2/3 [00:01<00:00,  1.94it/s]epoch = 3 | avg_train_loss = 2.208070755004883 | eval_loss = 1.916185975074768\n","epoch = 3 | avg_train_loss = 2.1146787915910994 | eval_loss = 1.916185975074768 \n","Done saved at epoch_3_checkpoint\n"]}],"source":["# distributed training with sample dataset\n","\n","# Fill in blank \"...\"\n","!DEBUG=true ON_DDP=true ON_MP=true torchrun --standalone --nproc_per_node=2 train.py"]},{"cell_type":"markdown","metadata":{},"source":["##### Train with full data"]},{"cell_type":"code","execution_count":12,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T09:50:03.877222Z","iopub.status.busy":"2024-02-25T09:50:03.876450Z","iopub.status.idle":"2024-02-25T14:16:58.146842Z","shell.execute_reply":"2024-02-25T14:16:58.145772Z","shell.execute_reply.started":"2024-02-25T09:50:03.877182Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["[2024-02-25 09:50:07,098] torch.distributed.run: [WARNING] master_addr is only used for static rdzv_backend and when rdzv_endpoint is not specified.\n","[2024-02-25 09:50:07,099] torch.distributed.run: [WARNING] \n","[2024-02-25 09:50:07,099] torch.distributed.run: [WARNING] *****************************************\n","[2024-02-25 09:50:07,099] torch.distributed.run: [WARNING] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \n","[2024-02-25 09:50:07,099] torch.distributed.run: [WARNING] *****************************************\n","2024-02-25 09:50:12.845581: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 09:50:12.845650: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 09:50:12.847315: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2024-02-25 09:50:12.857673: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 09:50:12.857734: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 09:50:12.859532: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","trainable params: 430080 || all params: 262794240 || trainable%: 0.16365655502951662\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Load dataset....\n","Generating train split: 25000 examples [00:00, 155840.74 examples/s]\n","Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Completed to load config & tokenizer\n","Load dataset....\n","Map: 100%|███████████████████████| 21250/21250 [00:15<00:00, 1380.66 examples/s]\n","Map: 100%|███████████████████████| 21250/21250 [00:16<00:00, 1305.50 examples/s]\n","Map: 100%|█████████████████████████| 3750/3750 [00:02<00:00, 1345.23 examples/s]\n","Creating json from Arrow format: 100%|████████████| 4/4 [00:00<00:00, 74.45ba/s]\n","Map: 100%|█████████████████████████| 3750/3750 [00:02<00:00, 1273.02 examples/s]\n","Creating json from Arrow format: 100%|████████████| 4/4 [00:00<00:00, 79.23ba/s]\n","epoch = 1 | avg_train_loss = 1.0315356613361244 | eval_loss = 0.8424947728157044\n","Done saved at epoch_1_checkpoint\n","epoch = 2 | avg_train_loss = 0.8674089198045613 | eval_loss = 0.8181960860331853\n","Done saved at epoch_2_checkpoint\n","Epoch 3 [Evaluation]: 100%|████████████████▉| 1870/1875 [15:19<00:02,  2.03it/s]epoch = 3 | avg_train_loss = 0.8375506941790143 | eval_loss = 0.8076259208202362\n","epoch = 3 | avg_train_loss = 0.8369833773383534 | eval_loss = 0.8076259208202362\n","Done saved at epoch_3_checkpoint\n"]}],"source":["# distributed training with full dataset\n","\n","# Fill in blank \"...\"\n","!ON_DDP=true ON_MP=true torchrun --standalone --nproc_per_node=2 train.py"]},{"cell_type":"markdown","metadata":{},"source":["### Inference\n","\n","Once the training phase concludes, we can utilize the subsequent code to evaluate our model and generate some instructions. Let's give it a try!"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-02-25T14:25:52.797785Z","iopub.status.busy":"2024-02-25T14:25:52.797387Z","iopub.status.idle":"2024-02-25T14:26:55.513020Z","shell.execute_reply":"2024-02-25T14:26:55.511957Z","shell.execute_reply.started":"2024-02-25T14:25:52.797746Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","2024-02-25 14:25:57.657559: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","2024-02-25 14:25:57.657619: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","2024-02-25 14:25:57.659250: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:392: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:397: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `128` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n","/opt/conda/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:407: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `40` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["My house is a green oasis in the middle of the city, with its lush garden full of colorful flowers and lush greenery providing a peaceful escape from the hustle and bustle of the outside world.\n","The teacher was able to explain the concept to the students because the teacher was well-versed in the material.\n","\n","The teacher was able to explain the concept to the students because the teacher was well-prepared.\n","\n","The teacher was able to explain the concept to the students because the teacher had a good understanding of the material.\n","\n","The family decided to go to the beach for their\n"]}],"source":["from inference import generate_inference\n","\n","model_path = 'TheBloke/phi-2-GPTQ'\n","\n","lora_weights_path = 'epoch_1_checkpoint' #you can edit it\n","instruction = 'Write about new house story'\n","user_inp = \"My house is green\" # TODO: fill input \n","\n","outputs = generate_inference(instruction=instruction, user_inp=user_inp, model_path=model_path, lora_weights_path=lora_weights_path)\n","print(outputs)"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":4}
